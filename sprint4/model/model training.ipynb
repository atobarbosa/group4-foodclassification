{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4da70169-0f90-48db-bc3f-2cec983b1001",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore as ms\n",
    "from mindcv.models import create_model\n",
    "from mindcv.loss import create_loss\n",
    "from mindcv.optim import create_optimizer\n",
    "from mindcv.data import create_transforms  # (kept if you still use it)\n",
    "from mindspore import context, ops\n",
    "from mindspore.dataset import ImageFolderDataset\n",
    "\n",
    "# NEW: extra vision ops for stronger augmentation + rescaling\n",
    "from mindspore.dataset.vision import (\n",
    "    Decode, Resize, Normalize, HWC2CHW, Rescale,\n",
    "    RandomHorizontalFlip, RandomErasing, Inter\n",
    ")\n",
    "from mindspore.dataset.transforms import TypeCast\n",
    "import mindspore.common.dtype as mstype\n",
    "\n",
    "import numpy as np, os, pathlib, json, time\n",
    "ms.set_seed(42); np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97c4b46a-5b79-4bde-9379-d705b12dff2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(6616:4660,MainProcess):2025-10-24-18:37:09.407.383 [mindspore\\context.py:1412] For 'context.set_context', the parameter 'device_target' will be deprecated and removed in a future version. Please use the api mindspore.set_device() instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MindSpore device target: CPU\n"
     ]
    }
   ],
   "source": [
    "# If you do have a CUDA/Ascend build, set that here; otherwise CPU is fine.\n",
    "device = \"GPU\" if \"GPU\" in (context.get_context(\"device_target\") or \"CPU\") else \"CPU\"\n",
    "context.set_context(mode=ms.GRAPH_MODE, device_target=device)\n",
    "print(\"MindSpore device target:\", context.get_context(\"device_target\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ff221d4-8cba-4244-89b2-8d4248f55366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['french_fries', 'hamburger', 'pancakes', 'pizza', 'sushi', 'tiramisu'] | num_classes: 6\n",
      "Train counts: {'french_fries': 200, 'hamburger': 200, 'pancakes': 200, 'pizza': 200, 'sushi': 200, 'tiramisu': 200}\n",
      "Val counts:   {'french_fries': 200, 'hamburger': 200, 'pancakes': 200, 'pizza': 200, 'sushi': 200, 'tiramisu': 200}\n",
      "✅ Loaded dataset: 1200 train, 1200 val samples.\n"
     ]
    }
   ],
   "source": [
    "train_dir = \"../train\"\n",
    "val_dir   = \"../val\"\n",
    "assert os.path.isdir(train_dir), f\"❌ Train directory not found: {train_dir}\"\n",
    "assert os.path.isdir(val_dir),   f\"❌ Validation directory not found: {val_dir}\"\n",
    "\n",
    "# Class order used everywhere (and saved with the checkpoint)\n",
    "classes = sorted([d for d in os.listdir(train_dir) if os.path.isdir(os.path.join(train_dir, d))])\n",
    "num_classes = len(classes)\n",
    "print(\"Classes:\", classes, \"| num_classes:\", num_classes)\n",
    "\n",
    "# Optional: show counts per class\n",
    "def count_images(root, cls):\n",
    "    p = os.path.join(root, cls)\n",
    "    return sum(1 for f in os.listdir(p) if f.lower().endswith((\".jpg\",\".jpeg\",\".png\")))\n",
    "print(\"Train counts:\", {c: count_images(train_dir, c) for c in classes})\n",
    "print(\"Val counts:  \", {c: count_images(val_dir, c) for c in classes})\n",
    "\n",
    "dataset_train = ImageFolderDataset(dataset_dir=train_dir, shuffle=True, decode=False)\n",
    "dataset_val   = ImageFolderDataset(dataset_dir=val_dir,   shuffle=False, decode=False)\n",
    "\n",
    "print(f\"✅ Loaded dataset: {dataset_train.get_dataset_size()} train, {dataset_val.get_dataset_size()} val samples.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9b649fa-cb21-4ac0-bd91-61742c650d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Datasets decoded, transformed, and batched.\n"
     ]
    }
   ],
   "source": [
    "mean = [0.485, 0.456, 0.406]\n",
    "std  = [0.229, 0.224, 0.225]\n",
    "batch_size = 16            # try 16 or 32 if memory allows\n",
    "num_workers = 4            # tune for your CPU cores\n",
    "\n",
    "# TRAIN transforms: decode → resize → flip → color jitter → rescale(0..1) → normalize → HWC2CHW → float32 → random erasing\n",
    "transforms_train = [\n",
    "    Decode(),\n",
    "    Resize((224, 224), interpolation=Inter.BICUBIC),\n",
    "    RandomHorizontalFlip(prob=0.5),\n",
    "    Rescale(1/255.0, 0.0),\n",
    "    Normalize(mean=mean, std=std),\n",
    "    HWC2CHW(),\n",
    "    TypeCast(mstype.float32),\n",
    "    RandomErasing(prob=0.25),\n",
    "]\n",
    "\n",
    "\n",
    "# VAL transforms: deterministic\n",
    "transforms_val = [\n",
    "    Decode(),\n",
    "    Resize((224, 224), interpolation=Inter.BICUBIC),\n",
    "    Rescale(1/255.0, 0.0),\n",
    "    Normalize(mean=mean, std=std),\n",
    "    HWC2CHW(),\n",
    "    TypeCast(mstype.float32),\n",
    "]\n",
    "\n",
    "dataset_train = dataset_train.map(operations=transforms_train, input_columns=\"image\",\n",
    "                                  num_parallel_workers=num_workers)\n",
    "dataset_train = dataset_train.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "dataset_val = dataset_val.map(operations=transforms_val, input_columns=\"image\",\n",
    "                              num_parallel_workers=num_workers)\n",
    "dataset_val = dataset_val.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "print(\"✅ Datasets decoded, transformed, and batched.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e20bbb7-7a73-4965-957e-e76e9a324b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(6616:4660,MainProcess):2025-10-24-18:37:59.610.598 [mindspore\\train\\serialization.py:1789] For 'load_param_into_net', 2 parameters in the 'net' are not loaded, because they are not in the 'parameter_dict', please check whether the network structure is consistent when training and loading checkpoint.\n",
      "[WARNING] ME(6616:4660,MainProcess):2025-10-24-18:37:59.611.603 [mindspore\\train\\serialization.py:1793] ['classifier.weight', 'classifier.bias'] are not loaded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model: ResNet50 with 6 classes\n"
     ]
    }
   ],
   "source": [
    "model = create_model(model_name=\"resnet50\", num_classes=num_classes, pretrained=True)\n",
    "model.set_train(True)\n",
    "print(\"✅ Model: ResNet50 with\", num_classes, \"classes\")\n",
    "\n",
    "# Optional (staged training): split params for head vs. backbone\n",
    "head_params = [p for p in model.trainable_params() if \"classifier\" in p.name]\n",
    "backbone_params = [p for p in model.trainable_params() if \"classifier\" not in p.name]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e4667bb-cd0d-4e8a-84ff-d347eed54dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Manual label smoothing setup -----\n",
    "num_classes = num_classes  # already defined earlier from your folders\n",
    "epsilon = 0.1              # smoothing factor\n",
    "\n",
    "# Loss takes *probability targets* (not sparse indices)\n",
    "loss_fn = ms.nn.SoftmaxCrossEntropyWithLogits(sparse=False, reduction='mean')\n",
    "\n",
    "# Convert class indices -> smoothed one-hot\n",
    "def smooth_labels(labels, num_classes, eps=0.1):\n",
    "    # one_hot returns float32 if on/off values are float32\n",
    "    on  = ms.Tensor(1.0, ms.float32)\n",
    "    off = ms.Tensor(0.0, ms.float32)\n",
    "    oh = ops.one_hot(labels, num_classes, on, off).astype(ms.float32)\n",
    "    return (1.0 - eps) * oh + eps / num_classes\n",
    "lr_head = 1e-3\n",
    "lr_full = 1e-4\n",
    "\n",
    "head_params = [p for p in model.trainable_params() if \"classifier\" in p.name]\n",
    "optimizer = create_optimizer(head_params if head_params else model.trainable_params(),\n",
    "                             opt=\"adamw\", lr=lr_head, weight_decay=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "40bdf8b1-b7b1-4f52-bc5e-9b2f9703d0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataset, loss_fn, optimizer):\n",
    "    model.set_train(True)\n",
    "    total_loss = ms.Tensor(0.0, ms.float32)\n",
    "    total_correct = ms.Tensor(0, ms.int32)\n",
    "    total_samples = ms.Tensor(0, ms.int32)\n",
    "\n",
    "    def forward_fn(x, y):\n",
    "        logits = model(x)\n",
    "        # y is class indices; convert to smoothed one-hot\n",
    "        y_smooth = smooth_labels(y, num_classes, epsilon).astype(ms.float32)\n",
    "        loss = loss_fn(logits, y_smooth)\n",
    "        return loss, logits\n",
    "    ...\n",
    "\n",
    "\n",
    "    grad_fn = ops.value_and_grad(forward_fn, None, optimizer.parameters, has_aux=True)\n",
    "\n",
    "    for batch in dataset.create_dict_iterator():\n",
    "        images, labels = batch[\"image\"], batch[\"label\"]\n",
    "        (loss, logits), grads = grad_fn(images, labels)\n",
    "        optimizer(grads)\n",
    "\n",
    "        preds = ops.Argmax(axis=1)(logits)\n",
    "        total_correct += ops.ReduceSum()(ops.Equal()(preds, labels).astype(ms.int32))\n",
    "        total_samples += labels.shape[0]\n",
    "        total_loss += loss\n",
    "\n",
    "    avg_loss = float((total_loss / total_samples.astype(ms.float32)).asnumpy())\n",
    "    acc = float((total_correct.astype(ms.float32) / total_samples.astype(ms.float32)).asnumpy())\n",
    "    return avg_loss, acc\n",
    "\n",
    "def validate(model, dataset):\n",
    "    model.set_train(False)\n",
    "    total_correct = 0; total_samples = 0\n",
    "    for batch in dataset.create_dict_iterator():\n",
    "        logits = model(batch[\"image\"])\n",
    "        preds = logits.asnumpy().argmax(axis=1)\n",
    "        labels = batch[\"label\"].asnumpy()\n",
    "        total_correct += (preds == labels).sum()\n",
    "        total_samples += labels.shape[0]\n",
    "    return total_correct / total_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "46167497-e969-4060-98d5-ebba7c51fe34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Training head for 3 epochs...\n",
      "[Head 1/3] Loss:0.0827 | Train:0.5792 | Val:0.7958 | 89.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(6616:4660,MainProcess):2025-10-24-18:46:14.797.165 [mindspore\\common\\api.py:125] The function \"after_grad\" at the file \"c:\\Users\\AJ\\anaconda3\\envs\\project\\lib\\site-packages\\mindspore\\ops\\composite\\base.py\", line 597 has been compiled again. Try to reuse the function object decorated by @jit to reduce the compile time. For more details, get instructions about `jit` at https://www.mindspore.cn/search?inputValue=jit. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Head 2/3] Loss:0.0607 | Train:0.7600 | Val:0.8508 | 83.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(6616:4660,MainProcess):2025-10-24-18:47:38.687.046 [mindspore\\common\\api.py:125] The function \"after_grad\" at the file \"c:\\Users\\AJ\\anaconda3\\envs\\project\\lib\\site-packages\\mindspore\\ops\\composite\\base.py\", line 597 has been compiled again. Try to reuse the function object decorated by @jit to reduce the compile time. For more details, get instructions about `jit` at https://www.mindspore.cn/search?inputValue=jit. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Head 3/3] Loss:0.0555 | Train:0.7958 | Val:0.8692 | 84.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(6616:4660,MainProcess):2025-10-24-18:49:03.512.603 [mindspore\\common\\api.py:125] The function \"after_grad\" at the file \"c:\\Users\\AJ\\anaconda3\\envs\\project\\lib\\site-packages\\mindspore\\ops\\composite\\base.py\", line 597 has been compiled again. Try to reuse the function object decorated by @jit to reduce the compile time. For more details, get instructions about `jit` at https://www.mindspore.cn/search?inputValue=jit. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔧 Fine-tuning all layers for 12 epochs...\n",
      "[Full 1/12] Loss:0.0495 | Train:0.8425 | Val:0.9858 | 246.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(6616:4660,MainProcess):2025-10-24-18:53:10.515.780 [mindspore\\common\\api.py:125] The function \"after_grad\" at the file \"c:\\Users\\AJ\\anaconda3\\envs\\project\\lib\\site-packages\\mindspore\\ops\\composite\\base.py\", line 597 has been compiled again. Try to reuse the function object decorated by @jit to reduce the compile time. For more details, get instructions about `jit` at https://www.mindspore.cn/search?inputValue=jit. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Full 2/12] Loss:0.0335 | Train:0.9800 | Val:0.9917 | 232.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(6616:4660,MainProcess):2025-10-24-18:57:02.744.092 [mindspore\\common\\api.py:125] The function \"after_grad\" at the file \"c:\\Users\\AJ\\anaconda3\\envs\\project\\lib\\site-packages\\mindspore\\ops\\composite\\base.py\", line 597 has been compiled again. Try to reuse the function object decorated by @jit to reduce the compile time. For more details, get instructions about `jit` at https://www.mindspore.cn/search?inputValue=jit. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Full 3/12] Loss:0.0307 | Train:0.9892 | Val:0.9967 | 273.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(6616:4660,MainProcess):2025-10-24-19:01:36.478.67 [mindspore\\common\\api.py:125] The function \"after_grad\" at the file \"c:\\Users\\AJ\\anaconda3\\envs\\project\\lib\\site-packages\\mindspore\\ops\\composite\\base.py\", line 597 has been compiled again. Try to reuse the function object decorated by @jit to reduce the compile time. For more details, get instructions about `jit` at https://www.mindspore.cn/search?inputValue=jit. \n",
      "[WARNING] ME(6616:4660,MainProcess):2025-10-24-19:05:26.549.263 [mindspore\\common\\api.py:125] The function \"after_grad\" at the file \"c:\\Users\\AJ\\anaconda3\\envs\\project\\lib\\site-packages\\mindspore\\ops\\composite\\base.py\", line 597 has been compiled again. Try to reuse the function object decorated by @jit to reduce the compile time. For more details, get instructions about `jit` at https://www.mindspore.cn/search?inputValue=jit. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Full 4/12] Loss:0.0290 | Train:0.9967 | Val:0.9967 | 230.5s\n",
      "[Full 5/12] Loss:0.0284 | Train:0.9983 | Val:0.9992 | 256.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(6616:4660,MainProcess):2025-10-24-19:09:43.145.33 [mindspore\\common\\api.py:125] The function \"after_grad\" at the file \"c:\\Users\\AJ\\anaconda3\\envs\\project\\lib\\site-packages\\mindspore\\ops\\composite\\base.py\", line 597 has been compiled again. Try to reuse the function object decorated by @jit to reduce the compile time. For more details, get instructions about `jit` at https://www.mindspore.cn/search?inputValue=jit. \n",
      "[WARNING] ME(6616:4660,MainProcess):2025-10-24-19:13:52.619.168 [mindspore\\common\\api.py:125] The function \"after_grad\" at the file \"c:\\Users\\AJ\\anaconda3\\envs\\project\\lib\\site-packages\\mindspore\\ops\\composite\\base.py\", line 597 has been compiled again. Try to reuse the function object decorated by @jit to reduce the compile time. For more details, get instructions about `jit` at https://www.mindspore.cn/search?inputValue=jit. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Full 6/12] Loss:0.0281 | Train:0.9983 | Val:0.9983 | 249.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(6616:4660,MainProcess):2025-10-24-19:18:09.944.864 [mindspore\\common\\api.py:125] The function \"after_grad\" at the file \"c:\\Users\\AJ\\anaconda3\\envs\\project\\lib\\site-packages\\mindspore\\ops\\composite\\base.py\", line 597 has been compiled again. Try to reuse the function object decorated by @jit to reduce the compile time. For more details, get instructions about `jit` at https://www.mindspore.cn/search?inputValue=jit. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Full 7/12] Loss:0.0280 | Train:0.9992 | Val:0.9975 | 257.3s\n",
      "[Full 8/12] Loss:0.0279 | Train:0.9992 | Val:1.0000 | 253.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(6616:4660,MainProcess):2025-10-24-19:22:23.108.053 [mindspore\\common\\api.py:125] The function \"after_grad\" at the file \"c:\\Users\\AJ\\anaconda3\\envs\\project\\lib\\site-packages\\mindspore\\ops\\composite\\base.py\", line 597 has been compiled again. Try to reuse the function object decorated by @jit to reduce the compile time. For more details, get instructions about `jit` at https://www.mindspore.cn/search?inputValue=jit. \n",
      "[WARNING] ME(6616:4660,MainProcess):2025-10-24-19:26:48.488.255 [mindspore\\common\\api.py:125] The function \"after_grad\" at the file \"c:\\Users\\AJ\\anaconda3\\envs\\project\\lib\\site-packages\\mindspore\\ops\\composite\\base.py\", line 597 has been compiled again. Try to reuse the function object decorated by @jit to reduce the compile time. For more details, get instructions about `jit` at https://www.mindspore.cn/search?inputValue=jit. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Full 9/12] Loss:0.0281 | Train:0.9975 | Val:0.9967 | 265.4s\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "could not create a primitive",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m epoch \u001b[38;5;241m=\u001b[39m num_epochs_head \u001b[38;5;241m+\u001b[39m e \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     23\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 24\u001b[0m train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m val_acc \u001b[38;5;241m=\u001b[39m validate(model, dataset_val)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Full \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs_full\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     27\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Train:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Val:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;241m-\u001b[39mt0\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[22], line 20\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, dataset, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39mcreate_dict_iterator():\n\u001b[0;32m     19\u001b[0m     images, labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m], batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m---> 20\u001b[0m     (loss, logits), grads \u001b[38;5;241m=\u001b[39m \u001b[43mgrad_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m     optimizer(grads)\n\u001b[0;32m     23\u001b[0m     preds \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mArgmax(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)(logits)\n",
      "File \u001b[1;32mc:\\Users\\AJ\\anaconda3\\envs\\project\\lib\\site-packages\\mindspore\\common\\api.py:1115\u001b[0m, in \u001b[0;36m_jit_ast.<locals>.wrap_func.<locals>.staging_specialize\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1112\u001b[0m     jit_graph_name \u001b[38;5;241m=\u001b[39m staging_specialize\u001b[38;5;241m.\u001b[39m__jit_graph_name__\n\u001b[0;32m   1113\u001b[0m jit_executor \u001b[38;5;241m=\u001b[39m _JitExecutor(\n\u001b[0;32m   1114\u001b[0m     func, hash_obj, \u001b[38;5;28;01mNone\u001b[39;00m, process_obj, jit_config, dynamic, jit_graph_name)\n\u001b[1;32m-> 1115\u001b[0m out \u001b[38;5;241m=\u001b[39m jit_executor(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(process_obj, ms\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCell):\n\u001b[0;32m   1117\u001b[0m     _clear_auto_parallel_context(process_obj)\n",
      "File \u001b[1;32mc:\\Users\\AJ\\anaconda3\\envs\\project\\lib\\site-packages\\mindspore\\common\\api.py:187\u001b[0m, in \u001b[0;36m_wrap_func.<locals>.wrapper\u001b[1;34m(*arg, **kwargs)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39marg, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 187\u001b[0m     results \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39marg, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _convert_python_data(results)\n",
      "File \u001b[1;32mc:\\Users\\AJ\\anaconda3\\envs\\project\\lib\\site-packages\\mindspore\\common\\api.py:676\u001b[0m, in \u001b[0;36m_JitExecutor.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    674\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    675\u001b[0m     _pynative_executor\u001b[38;5;241m.\u001b[39mclear_res()\n\u001b[1;32m--> 676\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mget_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecompile_only\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMS_DEV_PRECOMPILE_ONLY\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    679\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\AJ\\anaconda3\\envs\\project\\lib\\site-packages\\mindspore\\common\\api.py:672\u001b[0m, in \u001b[0;36m_JitExecutor.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    670\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    671\u001b[0m     _pynative_executor\u001b[38;5;241m.\u001b[39mset_jit_compile_status(\u001b[38;5;28;01mTrue\u001b[39;00m, phase)\n\u001b[1;32m--> 672\u001b[0m     phase \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompile(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;241m*\u001b[39margs_list, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    673\u001b[0m     _pynative_executor\u001b[38;5;241m.\u001b[39mset_jit_compile_status(\u001b[38;5;28;01mFalse\u001b[39;00m, phase)\n\u001b[0;32m    674\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\AJ\\anaconda3\\envs\\project\\lib\\site-packages\\mindspore\\common\\api.py:776\u001b[0m, in \u001b[0;36m_JitExecutor.compile\u001b[1;34m(self, method_name, *args, **kwargs)\u001b[0m\n\u001b[0;32m    773\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    774\u001b[0m     \u001b[38;5;66;03m# Set an attribute to fn as an identifier.\u001b[39;00m\n\u001b[0;32m    775\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(get_func(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__jit_function__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 776\u001b[0m     is_compile \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_graph_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompile_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mphase\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjit_config_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    777\u001b[0m     \u001b[38;5;28mdelattr\u001b[39m(get_func(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__jit_function__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    778\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: could not create a primitive"
     ]
    }
   ],
   "source": [
    "num_epochs_head = 3     # head-only warmup\n",
    "num_epochs_full = 12    # full fine-tune (total 15)\n",
    "best_acc, best_epoch = 0.0, -1\n",
    "save_dir = pathlib.Path(\"../models\"); save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"🚀 Training head for {num_epochs_head} epochs...\")\n",
    "for epoch in range(num_epochs_head):\n",
    "    t0 = time.time()\n",
    "    train_loss, train_acc = train_one_epoch(model, dataset_train, loss_fn, optimizer)\n",
    "    val_acc = validate(model, dataset_val)\n",
    "    print(f\"[Head {epoch+1}/{num_epochs_head}] \"\n",
    "          f\"Loss:{train_loss:.4f} | Train:{train_acc:.4f} | Val:{val_acc:.4f} | {time.time()-t0:.1f}s\")\n",
    "    if val_acc > best_acc:\n",
    "        best_acc, best_epoch = val_acc, epoch+1\n",
    "        ms.save_checkpoint(model, str(save_dir / \"resnet50_food_best.ckpt\"))\n",
    "\n",
    "# Switch optimizer to fine-tune ALL layers at a lower LR\n",
    "optimizer = create_optimizer(model.trainable_params(), opt=\"adamw\", lr=lr_full, weight_decay=1e-4)\n",
    "\n",
    "print(f\"\\n🔧 Fine-tuning all layers for {num_epochs_full} epochs...\")\n",
    "for e in range(num_epochs_full):\n",
    "    epoch = num_epochs_head + e + 1\n",
    "    t0 = time.time()\n",
    "    train_loss, train_acc = train_one_epoch(model, dataset_train, loss_fn, optimizer)\n",
    "    val_acc = validate(model, dataset_val)\n",
    "    print(f\"[Full {e+1}/{num_epochs_full}] \"\n",
    "          f\"Loss:{train_loss:.4f} | Train:{train_acc:.4f} | Val:{val_acc:.4f} | {time.time()-t0:.1f}s\")\n",
    "    if val_acc > best_acc:\n",
    "        best_acc, best_epoch = val_acc, epoch\n",
    "        ms.save_checkpoint(model, str(save_dir / \"resnet50_food_best.ckpt\"))\n",
    "\n",
    "print(f\"\\n✅ Best Val Acc: {best_acc:.4f} at epoch {best_epoch}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "70ca75b0-bdcf-425d-bae5-221e8a78ef70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Training complete. Checkpoint saved to ../models/resnet50_food.ckpt\n"
     ]
    }
   ],
   "source": [
    "# --- 8. Save Checkpoint ---\n",
    "os.makedirs(\"../models\", exist_ok=True)\n",
    "ms.save_checkpoint(model, \"../models/resnet50_food.ckpt\")\n",
    "print(\"✅ Training complete. Checkpoint saved to ../models/resnet50_food.ckpt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f58f732f-27e5-4d82-b85b-1a272c716c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved last checkpoint and labels.json\n"
     ]
    }
   ],
   "source": [
    "ms.save_checkpoint(model, \"../models/resnet50_food_last.ckpt\")\n",
    "with open(\"../models/labels.json\", \"w\") as f:\n",
    "    json.dump(classes, f, indent=2)\n",
    "print(\"Saved last checkpoint and labels.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.24"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
